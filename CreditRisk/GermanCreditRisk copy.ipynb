{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Report: German Credit Risk Analysis\n",
    "\n",
    "## Introduction\n",
    "The German Credit Risk Analysis project aims to predict credit risk using machine learning techniques. The dataset used in this project contains information about customers' credit applications, including demographic features, credit amount, duration, and risk classification.\n",
    "\n",
    "## Libraries\n",
    "- Pandas\n",
    "- NumPy\n",
    "- Matplotlib\n",
    "- Seaborn\n",
    "- LightGBM\n",
    "- Scikit-learn\n",
    "- Imbalanced-learn\n",
    "\n",
    "## Data Exploration and Preprocessing\n",
    "- Imported the dataset and preprocessed column names.\n",
    "- Explored data dimensions and target class balance.\n",
    "- Visualized numerical and categorical features.\n",
    "- Preprocessed the data by handling missing values, encoding categorical variables, and splitting the dataset into training and testing sets.\n",
    "\n",
    "## Model Training and Evaluation\n",
    "- Trained multiple machine learning models, including Logistic Regression, Decision Trees, Random Forest, and LightGBM.\n",
    "- Tuned hyperparameters using RandomizedSearchCV.\n",
    "- Evaluated models using various metrics such as accuracy, precision, recall, F1 score, and ROC AUC.\n",
    "- Plotted ROC curves, confusion matrices, and learning curves to analyze model performance.\n",
    "\n",
    "## Feature Importance Analysis\n",
    "- Conducted feature importance analysis using LightGBM.\n",
    "- Visualized feature importance to identify significant predictors in the model.\n",
    "\n",
    "## Model Deployment\n",
    "- Built a preprocessing pipeline to handle data transformations.\n",
    "- Applied the trained model to new data for credit risk prediction.\n",
    "- Appended predicted probabilities and classes to the dataset for further analysis.\n",
    "\n",
    "## Conclusion\n",
    "The German Credit Risk Analysis project successfully developed machine learning models to predict credit risk based on customer attributes. The LightGBM model demonstrated the best performance with an accuracy of XX% and an ROC AUC of XX%. Feature importance analysis identified key predictors influencing credit risk. The deployed model can be used for real-time credit risk assessment in financial institutions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries:\n",
    "\n",
    "This cell imports various standard libraries and modules necessary for data analysis, visualization, machine learning, and pipeline construction. Here's a breakdown of what each import statement does:\n",
    "\n",
    "- **pandas**: Used for data manipulation and analysis.\n",
    "- **numpy**: Provides support for large, multi-dimensional arrays and matrices, along with mathematical functions.\n",
    "- **matplotlib.pyplot**: A plotting library for creating static, interactive, and animated visualizations in Python.\n",
    "- **seaborn**: Built on top of matplotlib, seaborn provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "- **datetime**: Provides classes for manipulating dates and times.\n",
    "- **time**: Provides various time-related functions.\n",
    "- **matplotlib.gridspec**: Allows the creation of subplots with different sizes and alignments.\n",
    "\n",
    "Additionally, the code imports utility functions, custom transformers, and machine learning models:\n",
    "\n",
    "- **viz_utils**: Contains utility functions for data visualization.\n",
    "- **ml_utils**: Contains utility functions for machine learning tasks.\n",
    "- **custom_transformers**: Contains custom transformer classes for data preprocessing.\n",
    "- **Pipeline**: Enables constructing pipelines in scikit-learn for sequential execution of multiple data processing steps.\n",
    "- **ColumnTransformer**: Allows applying different transformations to different columns or subsets of data.\n",
    "- **StandardScaler**: Scales features by removing the mean and scaling to unit variance.\n",
    "- **SimpleImputer**: Imputes missing values using a specified strategy.\n",
    "- **LogisticRegression**: Implements logistic regression for binary classification.\n",
    "- **DecisionTreeClassifier**: Implements decision tree-based classifiers.\n",
    "- **RandomForestClassifier**: Implements random forest classifiers.\n",
    "- **lightgbm**: A gradient boosting framework that uses tree-based learning algorithms.\n",
    "- **train_test_split**: Splits data into random train and test subsets.\n",
    "- **RandomizedSearchCV**: Performs hyperparameter optimization using random search.\n",
    "- **cross_val_score**: Evaluates a score by cross-validation.\n",
    "- **classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score**: Functions for evaluating classification model performance metrics.\n",
    "- **RandomUnderSampler, SMOTE**: Techniques for handling imbalanced datasets by undersampling the majority class or oversampling the minority class, respectively.\n",
    "\n",
    "These imports set up the environment for data analysis, preprocessing, modeling, and evaluation. They provide the necessary tools and functionality to perform various tasks in a machine learning project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:05.975826Z",
     "start_time": "2020-07-01T00:12:59.022046Z"
    }
   },
   "outputs": [],
   "source": [
    "# Stsandard libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "import time\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Utilities\n",
    "from utils.viz_utils import *\n",
    "from utils.ml_utils import *\n",
    "from utils.custom_transformers import *\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Modeling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, cross_val_predict, \\\n",
    "                                    learning_curve\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, \\\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomFunctions:\n",
    "\n",
    "The `catplot_percentage_analysis` function is designed to visualize categorical data by plotting the percentage distribution of each category with respect to a specified target variable (`hue`). Here's an overview of what the function does:\n",
    "\n",
    "- **Purpose**: To analyze the distribution and proportion of categorical variables in a dataset concerning a specific target variable.\n",
    "\n",
    "- **Steps**:\n",
    "  1. Retrieve the categorical variables from the dataset.\n",
    "  2. Set up parameters for plotting, such as the number of columns for the matplotlib figure (`fig_cols`).\n",
    "  3. Apply loops to generate plots and format them.\n",
    "\n",
    "- **Arguments**:\n",
    "  - `df_categorical`: The dataset containing categorical variables to be analyzed (pandas.DataFrame).\n",
    "  - `hue`: The target variable used for stratification and color differentiation in the plots.\n",
    "  - `fig_cols`: The number of columns in the matplotlib figure (integer, default is 2).\n",
    "  - `palette`: The color palette to be used in the plots (string, default is 'viridis').\n",
    "  - `figsize`: The size of the matplotlib figure (tuple, default is (16, 10)).\n",
    "\n",
    "- **Return**: None (The function generates and displays matplotlib plots directly).\n",
    "\n",
    "The function utilizes seaborn for styling and plotting. It creates horizontal bar plots for each categorical variable, representing the percentage distribution of categories with respect to the target variable (`hue`). The plots are organized in a grid layout based on the specified number of columns (`fig_cols`). Empty subplot spaces are handled to ensure a clean and organized visualization.\n",
    "\n",
    "Overall, this function provides a convenient way to explore the relationship between categorical variables and the target variable in a dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catplot_percentage_analysis(df_categorical, hue, fig_cols=2, palette='viridis', figsize=(16, 10)):\n",
    "\n",
    "    sns.set(style='white', palette='muted', color_codes=True)\n",
    "    sns.set_palette(palette)\n",
    "    cat_features = list(df_categorical.drop(hue, axis=1).columns)\n",
    "    total_cols = len(cat_features)\n",
    "    fig_rows = ceil(total_cols / fig_cols)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows=fig_rows, ncols=fig_cols, figsize=(figsize))\n",
    "    i, j = 0, 0\n",
    "\n",
    "    for col in cat_features:\n",
    "        try:\n",
    "            ax = axs[i, j]\n",
    "        except:\n",
    "            ax = axs[j]\n",
    "\n",
    "        col_to_hue = pd.crosstab(df_categorical[col], df_categorical[hue])\n",
    "        col_to_hue.div(col_to_hue.sum(1).astype(float), axis=0).plot(kind='barh', stacked=True, ax=ax)\n",
    "\n",
    "        format_spines(ax, right_border=False)\n",
    "        ax.set_title(col)\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "        j += 1\n",
    "        if j == fig_cols:\n",
    "            j = 0\n",
    "            i += 1\n",
    "\n",
    "    i, j = (0, 0)\n",
    "    for n_plots in range(fig_rows * fig_cols):\n",
    "\n",
    "        if n_plots >= len(cat_features):\n",
    "            try:\n",
    "                axs[i][j].axis('off')\n",
    "            except TypeError as e:\n",
    "                axs[j].axis('off')\n",
    "\n",
    "        j += 1\n",
    "        if j == fig_cols:\n",
    "            j = 0\n",
    "            i += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:06.056489Z",
     "start_time": "2020-07-01T00:13:05.980813Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Data path\n",
    "df_ori = pd.read_csv('german_credit_data.csv')\n",
    "df = df_ori.iloc[:, 1:]\n",
    "df.columns = [col.lower().strip().replace(' ', '_') for col in df.columns]\n",
    "\n",
    "# Results\n",
    "print(f'Data dimension: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:06.255685Z",
     "start_time": "2020-07-01T00:13:06.062157Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Target class balance\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "label_names = ['Good', 'Bad']\n",
    "color_list = ['navy', 'mediumvioletred']\n",
    "text = f'Total\\n{len(df_ori)}'\n",
    "title = 'Target Class Balance'\n",
    "\n",
    "# Visualizing it through a donut chart\n",
    "donut_plot(df, col='risk', ax=ax, label_names=label_names, colors=color_list, title=title, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:06.318341Z",
     "start_time": "2020-07-01T00:13:06.257682Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Overview from the data\n",
    "df_overview = data_overview(df)\n",
    "df_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:09.305128Z",
     "start_time": "2020-07-01T00:13:06.320269Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "num_cols = ['age', 'credit_amount', 'duration']\n",
    "color_sequence = ['navy', 'mediumseagreen', 'navy']\n",
    "numplot_analysis(df[num_cols], color_sequence=color_sequence, hist=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:11.497759Z",
     "start_time": "2020-07-01T00:13:09.30879Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "num_cols += ['risk']\n",
    "numplot_analysis(df[num_cols], hue='risk', color_hue=color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:13.573284Z",
     "start_time": "2020-07-01T00:13:11.500925Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "boxenplot(df, features=['age', 'credit_amount', 'duration'], hue='risk', fig_cols=3, figsize=(15, 5), \n",
    "          palette=color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:18.242953Z",
     "start_time": "2020-07-01T00:13:13.577298Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "cat_features = [col for col, dtype in df.dtypes.items() if dtype == 'object']\n",
    "catplot_analysis(df[cat_features], palette='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:22.082453Z",
     "start_time": "2020-07-01T00:13:18.249931Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "catplot_analysis(df[cat_features], hue='risk', palette=color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:25.639683Z",
     "start_time": "2020-07-01T00:13:22.088723Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "rev_color_list = ['mediumvioletred', 'navy']\n",
    "catplot_percentage_analysis(df[cat_features], hue='risk', palette=rev_color_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:27.192704Z",
     "start_time": "2020-07-01T00:13:25.642677Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "mean_sum_analysis(df, group_col='purpose', value_col='credit_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:29.01606Z",
     "start_time": "2020-07-01T00:13:27.202347Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "mean_sum_analysis(df, group_col='purpose', value_col='duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:29.607709Z",
     "start_time": "2020-07-01T00:13:29.020576Z"
    },
    "_kg_hide-input": true,
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "gender_palette = ['cornflowerblue', 'salmon']\n",
    "mean_sum_analysis(df, group_col='sex', value_col='credit_amount', orient='horizontal', \n",
    "                  palette=gender_palette, figsize=(12, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:38.81555Z",
     "start_time": "2020-07-01T00:13:29.609707Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df[num_cols], hue='risk', palette=color_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:38.947788Z",
     "start_time": "2020-07-01T00:13:38.817576Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "amount_risk = df.groupby(by='risk', as_index=False).sum().loc[:, ['risk', 'credit_amount']]\n",
    "amount_risk['percentage'] = amount_risk['credit_amount'] / amount_risk['credit_amount'].sum()\n",
    "amount_risk.style.background_gradient(cmap='Reds_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:39.228564Z",
     "start_time": "2020-07-01T00:13:38.950167Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Creating figure\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "\n",
    "# Defining useful elements for the donut chart\n",
    "values = amount_risk['credit_amount']\n",
    "labels = amount_risk['risk']\n",
    "center_circle = plt.Circle((0, 0), 0.8, color='white')\n",
    "\n",
    "# Plotting the pizza chart and the center circle\n",
    "ax.pie(values, labels=labels, colors=['darkred', 'cadetblue'], autopct=make_autopct(values))\n",
    "ax.add_artist(center_circle)\n",
    "\n",
    "kwargs = dict(size=20, fontweight='bold', va='center')\n",
    "ax.text(0, 0, f'Total Amount\\n${values.sum()}', ha='center', **kwargs)\n",
    "ax.set_title('Credit Amount Made Available to Customers by Risk', size=14, color='dimgrey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:41.146464Z",
     "start_time": "2020-07-01T00:13:39.230562Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Creating figure\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "\n",
    "# Scatterplot\n",
    "sns.scatterplot(x='age', y='credit_amount', data=df, hue='housing', ax=axs[0], palette='magma', alpha=.8)\n",
    "sns.scatterplot(x='age', y='credit_amount', data=df, hue='job', ax=axs[1], palette='YlGnBu')\n",
    "\n",
    "# Customizing plot\n",
    "format_spines(axs[0], right_border=False)\n",
    "format_spines(axs[1], right_border=False)\n",
    "axs[0].set_title('Credit Amomunt and Age Distribution by Housing', size=12, color='dimgrey')\n",
    "axs[1].set_title('Credit Amomunt and Age Distribution by Job', size=12, color='dimgrey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:45.563589Z",
     "start_time": "2020-07-01T00:13:43.405875Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "g = (sns.jointplot(x='credit_amount', y='duration', data=df, color='seagreen', kind='hex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:46.301868Z",
     "start_time": "2020-07-01T00:13:45.564586Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Creating new categories for duration col\n",
    "bins = [0, 10, 30, 50, np.inf]\n",
    "labels = ['<= 10', 'between 10 and 30', 'between 30 and 50', '> 50']\n",
    "df['cat_duration'] = pd.cut(df['duration'], bins=bins, labels=labels)\n",
    "\n",
    "# Creating figure\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Scatterplot\n",
    "sns.scatterplot(x='age', y='credit_amount', data=df, hue='cat_duration', palette='YlGnBu')\n",
    "\n",
    "# Customizing plot\n",
    "format_spines(ax, right_border=False)\n",
    "ax.set_title('Credit Amomunt and Age Distribution by Duration Category', size=14, color='dimgrey')\n",
    "ax.legend(loc='upper right', fancybox=False, framealpha=0.2)\n",
    "df.drop('cat_duration', axis=1, inplace=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T15:26:13.69069Z",
     "start_time": "2020-07-05T15:26:12.507807Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Definindo melhor número de clusters\n",
    "columns = ['age', 'duration']\n",
    "cluster_data = df.loc[:, columns]\n",
    "K_min, K_max = 1, 8\n",
    "\n",
    "# Call the elbow_method_kmeans1 function\n",
    "elbow_method_kmeans1(cluster_data, K_min, K_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=3)\n",
    "cluster_data\n",
    "model.fit(cluster_data)\n",
    "plot_kmeans_clusters_2d(cluster_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T15:39:12.082594Z",
     "start_time": "2020-07-05T15:39:11.091088Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# Definindo melhor número de clusters\n",
    "columns = ['credit_amount', 'duration']\n",
    "cluster_data = df.loc[:, columns]\n",
    "K_min, K_max = 1, 8\n",
    "elbow_method_kmeans1(cluster_data, K_min, K_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-05T15:39:17.811945Z",
     "start_time": "2020-07-05T15:39:17.17746Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2)\n",
    "cluster_data\n",
    "model.fit(cluster_data)\n",
    "plot_kmeans_clusters_2d(cluster_data, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:46.333104Z",
     "start_time": "2020-07-01T00:13:46.303804Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a target column\n",
    "df['target'] = df['risk'].apply(lambda x: 1 if x == 'bad' else 0)\n",
    "df.drop('risk', axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:46.356508Z",
     "start_time": "2020-07-01T00:13:46.33599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Building the preprocessing Pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('dup_dropped', DropDuplicates()),\n",
    "    ('data_splitter', SplitData(target='target'))\n",
    "])\n",
    "\n",
    "# Applying this pipeline\n",
    "X_train, X_test, y_train, y_test = preprocessing_pipeline.fit_transform(df)\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:46.381772Z",
     "start_time": "2020-07-01T00:13:46.365813Z"
    }
   },
   "outputs": [],
   "source": [
    "# Splitting the data by dtype\n",
    "num_features = [col for col, dtype in X_train.dtypes.items() if dtype != 'object']\n",
    "cat_features = [col for col, dtype in X_train.dtypes.items() if dtype == 'object']\n",
    "\n",
    "# Building a numerical pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Building a categorical pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('encoder', DummiesEncoding(dummy_na=True))\n",
    "])\n",
    "\n",
    "# Building a complete pipeline\n",
    "full_pipeline = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_features),\n",
    "    ('cat', cat_pipeline, cat_features)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:46.472744Z",
     "start_time": "2020-07-01T00:13:46.389782Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the data prep pipeline\n",
    "X_train_prep = full_pipeline.fit_transform(X_train)\n",
    "X_test_prep = full_pipeline.fit_transform(X_test)\n",
    "\n",
    "print(f'Shape of X_train_prep: {X_train_prep.shape}')\n",
    "print(f'Shape of X_test_prep: {X_test_prep.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:46.533489Z",
     "start_time": "2020-07-01T00:13:46.47519Z"
    }
   },
   "outputs": [],
   "source": [
    "# Returning the final features of the dataset\n",
    "encoded_features = full_pipeline.named_transformers_['cat']['encoder'].features_after_encoding\n",
    "model_features = num_features + encoded_features\n",
    "df_train_prep = pd.DataFrame(X_train_prep, columns=model_features)\n",
    "df_train_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:46.557505Z",
     "start_time": "2020-07-01T00:13:46.536481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression hyperparameters\n",
    "logreg_param_grid = {\n",
    "    'C': np.linspace(0.1, 10, 20),\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'random_state': [42],\n",
    "    'solver': ['liblinear']\n",
    "}\n",
    "\n",
    "# Decision Trees hyperparameters\n",
    "tree_param_grid = {\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_depth': [3, 5, 10, 20],\n",
    "    'max_features': np.arange(1, X_train.shape[1]),\n",
    "    'class_weight': ['balanced', None],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "# Random Forest hyperparameters\n",
    "forest_param_grid = {\n",
    "    'bootstrap': [True, False],\n",
    "    'max_depth': [3, 5, 10, 20, 50],\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'random_state': [42],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "# LightGBM hyperparameters\n",
    "lgbm_param_grid = {\n",
    "    'num_leaves': list(range(8, 92, 4)),\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [3, 4, 5, 6, 8, 12, 16],\n",
    "    'learning_rate': [0.1, 0.05, 0.01, 0.005],\n",
    "    'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.95, 10),\n",
    "}\n",
    "\n",
    "lgbm_fixed_params = {\n",
    "    'application': 'binary',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'is_unbalance': 'true',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'feature_fraction': 0.5,\n",
    "    'bagging_fraction': 0.5,\n",
    "    'bagging_freq': 20,\n",
    "    'learning_rate': 0.05,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:13:46.568988Z",
     "start_time": "2020-07-01T00:13:46.559502Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up classifiers\n",
    "set_classifiers = {\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(),\n",
    "        'params': logreg_param_grid\n",
    "    },\n",
    "    'DecisionTrees': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': tree_param_grid\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': forest_param_grid\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'model': lgb.LGBMClassifier(**lgbm_fixed_params),\n",
    "        'params': lgbm_param_grid\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:14:06.370876Z",
     "start_time": "2020-07-01T00:13:46.57198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating an instance for the homemade class BinaryClassifiersAnalysis\n",
    "clf_tool = BinaryClassifiersAnalysis()\n",
    "clf_tool.fit(set_classifiers, X_train_prep, y_train, random_search=True, cv=5, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:14:35.04338Z",
     "start_time": "2020-07-01T00:14:06.373896Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating metrics\n",
    "df_performances = clf_tool.evaluate_performance(X_train_prep, y_train, X_test_prep, y_test, cv=5)\n",
    "df_performances.reset_index(drop=True).style.background_gradient(cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T01:22:38.570874Z",
     "start_time": "2020-07-03T01:22:37.769918Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 11))\n",
    "lgbm_feature_importance = clf_tool.feature_importance_analysis(model_features, specific_model='LightGBM', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T01:22:39.568145Z",
     "start_time": "2020-07-03T01:22:38.572867Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "clf_tool.plot_roc_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-03T01:22:46.219948Z",
     "start_time": "2020-07-03T01:22:39.57014Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "clf_tool.plot_confusion_matrix(classes=['Good', 'Bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T00:06:17.196544Z",
     "start_time": "2020-07-04T00:06:13.619885Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "clf_tool.plot_learning_curve('LightGBM', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T00:06:17.644083Z",
     "start_time": "2020-07-04T00:06:17.199502Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "clf_tool.plot_score_distribution('LightGBM', shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-04T00:06:25.421521Z",
     "start_time": "2020-07-04T00:06:24.108077Z"
    },
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "clf_tool.plot_score_bins(model_name='LightGBM', bin_range=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-07T16:39:42.080588Z",
     "start_time": "2020-07-07T16:39:41.955994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libs\n",
    "import pandas as pd\n",
    "from utils.custom_transformers import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Reading the raw data\n",
    "df_ori = import_data('german_credit_data.csv', optimized=True)\n",
    "df = df_ori.iloc[:, 1:]\n",
    "df.columns = [col.lower().strip().replace(' ', '_') for col in df.columns]\n",
    "df['target'] = df['risk'].apply(lambda x: 1 if x == 'bad' else 0)\n",
    "df.drop('risk', axis=1, inplace=True)\n",
    "\n",
    "# Applying the data prep pipeline (the pkl file could be read from a specific path)\n",
    "scoring_data = full_pipeline.fit_transform(df)\n",
    "\n",
    "# Using the trained model for predicting (the pkl file could be read from a specific path)\n",
    "model = clf_tool.classifiers_info['LightGBM']['estimator']\n",
    "y_pred = model.predict(scoring_data)\n",
    "y_score = model.predict_proba(scoring_data)[:, 1]\n",
    "\n",
    "# Appending the predictions to the data\n",
    "df['y_score'] = y_score\n",
    "df['y_pred'] = y_pred\n",
    "df['y_class'] = df['y_pred'].apply(lambda x: 'Bad' if x == 1 else 'Good')\n",
    "\n",
    "# Creating bins\n",
    "bins = df['y_score'].quantile(np.arange(0, 1.01, 0.1)).values\n",
    "labels = ['Faixa ' + str(i) for i in range(len(bins)-1, 0, -1)]\n",
    "df['faixa'] = pd.cut(df['y_score'], bins=bins, labels=labels, include_lowest=True)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
